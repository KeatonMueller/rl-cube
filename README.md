# rl-cube
This is my attempt to use reinforcement learning to solve a Rubik's Cube without the aid of human knowledge.

I'm working on this project as a part of CPSC 674: Advanced Computational Intelligence for Games.

Many of the ideas I will draw upon and try to recreate come from these two papers:

	https://arxiv.org/abs/1805.07470

	https://www.nature.com/articles/s42256-019-0070-z#Sec11

## How it Works
This project uses a technique introduced by Stephen McAleer called Autodidactic Iteration.

First a joint policy and value network is trained via reinforcement learning.
Training examples are generated by randomly scrambling Rubik's Cubes, and
their labels are created using the network being trained. After a period of time,
new training samples are generated and labelled with the updated parameters.
This process is repeated several times.

Once the policy and value network is trained, it is used to guide a Monte-Carlo
Tree Search to go from the scrambled to the solved state.

## How to Use It
You need Python 3 and PyTorch to run this project.

To see a list of options for this project, clone this repo and run


`python3 driver.py -h`

To type in scrambles for a pre-trained model to attempt to solve, run something like

`python3 driver.py --load model_500 --solve`

Note that as of this time, the model has only been trained on scrambles
of length 4. It can reliably solve all 4-move scrambles in under 5 seconds
with an average time of 0.16 seconds (time will vary depending on machine it's
run on), but it has not yet been trained to handle scrambles of arbitrary length.
